{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d1fe2a5-416d-47b6-afa4-aa5a98ba9249",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm # Progress bar\n",
    "\n",
    "from fuzzywuzzy import process\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fcd260b-25e6-4af2-ad9e-6db0b7656ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_characters_genders_context=pd.read_csv('DATA/output_characters_genders_context.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "verb_tags=['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "adj_tags=['JJ','JJR','JJS']\n",
    "noun_tags=['NN','NNS','NNP','NNPS']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with empty 'associated words' column: 10513\n"
     ]
    }
   ],
   "source": [
    "# Count number of rows with missing associated_words\n",
    "empty_rows = output_characters_genders_context[output_characters_genders_context['associated_words'].isnull() | (output_characters_genders_context['associated_words'] == '')]\n",
    "count_empty_rows = empty_rows.shape[0]\n",
    "print(f\"Number of rows with empty 'associated words' column: {count_empty_rows}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "dfc8f1f4-b6b5-4007-ad6a-321f38e28d62",
   "metadata": {},
   "source": [
    "Modif initial fct to keep the id column for later merges "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3497bda-f543-47cc-b71f-18117a0d20fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_words(df, id_col, char_name_col, to_extract):\n",
    "    tokens = pd.Series()\n",
    "    tagged_tokens = []\n",
    "    chunks_array = []\n",
    "    verbs_list = []\n",
    "    adjs_list = []\n",
    "    nouns_list = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Create tqdm progress bar for the loop\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Movies\"):\n",
    "        verbs = []\n",
    "        adjs = []\n",
    "        nouns = []\n",
    "        text = row[to_extract]\n",
    "        associated_w_text = row[char_name_col]\n",
    "        movie_id = row[id_col]\n",
    "        \n",
    "        if type(text) == str:  # To only keep movies with a summary (ignoring NaN)\n",
    "            token = [word for word in nltk.word_tokenize(text) if word.lower() not in stop_words]  # Removing stopwords\n",
    "            tokens[associated_w_text] = token\n",
    "            tagged_tokens.append((movie_id, associated_w_text, nltk.pos_tag(token)))\n",
    "\n",
    "    # Tqdm progress bar for the second loop\n",
    "    for movie_id, associated_w_text, tagged_token in tqdm(tagged_tokens, desc=\"Processing Tokens\", leave=False):\n",
    "        chunks_array.append((movie_id, associated_w_text, nltk.ne_chunk(tagged_token)))\n",
    "\n",
    "        verbs = []\n",
    "        adjs = []\n",
    "        nouns = []\n",
    "\n",
    "        # Categorize\n",
    "        for word, pos_tag in tagged_token:\n",
    "            if pos_tag in verb_tags:\n",
    "                verbs.append(word)\n",
    "            elif pos_tag in adj_tags:\n",
    "                adjs.append(word)\n",
    "            elif pos_tag in noun_tags:\n",
    "                nouns.append(word)\n",
    "\n",
    "        verbs_list.append((movie_id, associated_w_text, verbs))\n",
    "        adjs_list.append((movie_id, associated_w_text, adjs))\n",
    "        nouns_list.append((movie_id, associated_w_text, nouns))\n",
    "\n",
    "    # Returns lists of all verbs, adjectives, and nouns for each movie and raw chunks for each movie\n",
    "    return verbs_list, adjs_list, nouns_list, chunks_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f4bd861-6ba2-4817-9be3-6eb0e0041778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Movies: 100%|██████████| 176334/176334 [18:12<00:00, 161.36it/s] \n",
      "                                                                            \r"
     ]
    }
   ],
   "source": [
    "verbs, adjs, nouns, chunks = extract_words(output_characters_genders_context, \"IMDB_ID\", \"character_name\",\"associated_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b207a5ba-6a7e-4313-9c3f-cd0e4a2ceceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames from the lists\n",
    "verbs_df = pd.DataFrame(verbs, columns=['IMDB_ID', 'character_name', 'Verbs'])\n",
    "adjs_df = pd.DataFrame(adjs, columns=['IMDB_ID', 'character_name', 'Adjectives'])\n",
    "nouns_df = pd.DataFrame(nouns, columns=['IMDB_ID', 'character_name', 'Nouns'])\n",
    "chunks_df = pd.DataFrame(chunks, columns=['IMDB_ID', 'character_name', 'Chunks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e50af8cb-758d-40b7-b58d-76430218efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames on 'IMDB_ID' and 'character_name'\n",
    "final_df = verbs_df.merge(adjs_df, on=['IMDB_ID', 'character_name']) \\\n",
    "                    .merge(nouns_df, on=['IMDB_ID', 'character_name']) \\\n",
    "                    .merge(chunks_df, on=['IMDB_ID', 'character_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3844eb29-89af-489b-aa8e-1d2580138195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to have the genders, use that df if it is too big to merge with the character_data and save to csv\n",
    "# DO NOT RUN THAT CELL IF RUN THE NEXT ONE BECAUSE THE GENDER COLUMN WILL BE DUPLICATED\n",
    "final_df = pd.merge(final_df, output_characters_genders_context[['IMDB_ID', 'character_name', 'gender']], on=['IMDB_ID', 'character_name'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "          IMDB_ID      character_name  \\\n0       tt0228333  Sgt Jericho Butler   \n1       tt0228333     Bashira Kincaid   \n2       tt0228333    Michael Descanso   \n3       tt0228333      Big Daddy Mars   \n4       tt0228333            Akooshay   \n...           ...                 ...   \n165816  tt9913288       Trent Osborne   \n165817  tt9914522           Mackenzie   \n165818  tt9914522          Evan's dad   \n165819  tt9914522                Jade   \n165820  tt9914522                Evan   \n\n                                                    Verbs  \\\n0       [walk, wearing, sent, opened, released, posses...   \n1       [killed, returning, blame, cot, escapes, leaving]   \n2       [planet, finds, missing, discovered, discovere...   \n3                                   [planet, terraformed]   \n4                            [discovered, created, wiped]   \n...                                                   ...   \n165816                 [come, mail, mail, mail, unmarked]   \n165817             [murdered, regarding, make, regarding]   \n165818                       [make, regarding, regarding]   \n165819                                             [make]   \n165820                                  [make, regarding]   \n\n                                               Adjectives  \\\n0       [second, pick, disembodied, possible, Unfortun...   \n1                                              [massacre]   \n2       [second, 22nd, 22nd, doorway, ancient, Martian...   \n3                                                      []   \n4                         [second, 22nd, ancient, fierce]   \n...                                                   ...   \n165816                             [piece, unmarked, red]   \n165817                                       [new, niece]   \n165818                                              [new]   \n165819                                              [new]   \n165820                                              [new]   \n\n                                                    Nouns  \\\n0       [half, humans, surface, pressure, suits, team,...   \n1       [pick, transport, prisoner, Desolation, Willia...   \n2       [Set, century, film, century, film, Mars, mini...   \n3                                [century, film, depicts]   \n4       [Set, century, film, miners, Martian, miners, ...   \n...                                                   ...   \n165816  [day, home, bills, get, bills, mail, stands, s...   \n165817  [Holden, sister, brother, law, husband, Evan, ...   \n165818        [Mackenzie, husband, decision, make, niece]   \n165819  [honeymoon, Mackenzie, Evan, decision, make, r...   \n165820                     [Mackenzie, husband, decision]   \n\n                                                   Chunks gender  \n0       [(second, JJ), (half, NN), (22nd, CD), (humans...      M  \n1       [(pick, NN), (transport, NN), (prisoner, NN), ...      F  \n2       [[(Set, NNP)], (second, JJ), (22nd, JJ), (cent...      M  \n3       [(century, NN), (film, NN), (depicts, NNS), (p...      M  \n4       [[(Set, NNP)], (second, JJ), (22nd, JJ), (cent...      F  \n...                                                   ...    ...  \n165816  [(day, NN), (come, VB), (home, NN), (bills, NN...      M  \n165817  [[(Holden, NNP)], (sister, NN), (brother, NN),...      F  \n165818  [[(Mackenzie, NNP)], (new, JJ), (husband, NN),...      M  \n165819  [(honeymoon, NN), [(Mackenzie, NNP)], (new, JJ...      F  \n165820  [[(Mackenzie, NNP)], (new, JJ), (husband, NN),...      M  \n\n[165821 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>IMDB_ID</th>\n      <th>character_name</th>\n      <th>Verbs</th>\n      <th>Adjectives</th>\n      <th>Nouns</th>\n      <th>Chunks</th>\n      <th>gender</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>tt0228333</td>\n      <td>Sgt Jericho Butler</td>\n      <td>[walk, wearing, sent, opened, released, posses...</td>\n      <td>[second, pick, disembodied, possible, Unfortun...</td>\n      <td>[half, humans, surface, pressure, suits, team,...</td>\n      <td>[(second, JJ), (half, NN), (22nd, CD), (humans...</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>tt0228333</td>\n      <td>Bashira Kincaid</td>\n      <td>[killed, returning, blame, cot, escapes, leaving]</td>\n      <td>[massacre]</td>\n      <td>[pick, transport, prisoner, Desolation, Willia...</td>\n      <td>[(pick, NN), (transport, NN), (prisoner, NN), ...</td>\n      <td>F</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>tt0228333</td>\n      <td>Michael Descanso</td>\n      <td>[planet, finds, missing, discovered, discovere...</td>\n      <td>[second, 22nd, 22nd, doorway, ancient, Martian...</td>\n      <td>[Set, century, film, century, film, Mars, mini...</td>\n      <td>[[(Set, NNP)], (second, JJ), (22nd, JJ), (cent...</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>tt0228333</td>\n      <td>Big Daddy Mars</td>\n      <td>[planet, terraformed]</td>\n      <td>[]</td>\n      <td>[century, film, depicts]</td>\n      <td>[(century, NN), (film, NN), (depicts, NNS), (p...</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>tt0228333</td>\n      <td>Akooshay</td>\n      <td>[discovered, created, wiped]</td>\n      <td>[second, 22nd, ancient, fierce]</td>\n      <td>[Set, century, film, miners, Martian, miners, ...</td>\n      <td>[[(Set, NNP)], (second, JJ), (22nd, JJ), (cent...</td>\n      <td>F</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>165816</th>\n      <td>tt9913288</td>\n      <td>Trent Osborne</td>\n      <td>[come, mail, mail, mail, unmarked]</td>\n      <td>[piece, unmarked, red]</td>\n      <td>[day, home, bills, get, bills, mail, stands, s...</td>\n      <td>[(day, NN), (come, VB), (home, NN), (bills, NN...</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>165817</th>\n      <td>tt9914522</td>\n      <td>Mackenzie</td>\n      <td>[murdered, regarding, make, regarding]</td>\n      <td>[new, niece]</td>\n      <td>[Holden, sister, brother, law, husband, Evan, ...</td>\n      <td>[[(Holden, NNP)], (sister, NN), (brother, NN),...</td>\n      <td>F</td>\n    </tr>\n    <tr>\n      <th>165818</th>\n      <td>tt9914522</td>\n      <td>Evan's dad</td>\n      <td>[make, regarding, regarding]</td>\n      <td>[new]</td>\n      <td>[Mackenzie, husband, decision, make, niece]</td>\n      <td>[[(Mackenzie, NNP)], (new, JJ), (husband, NN),...</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>165819</th>\n      <td>tt9914522</td>\n      <td>Jade</td>\n      <td>[make]</td>\n      <td>[new]</td>\n      <td>[honeymoon, Mackenzie, Evan, decision, make, r...</td>\n      <td>[(honeymoon, NN), [(Mackenzie, NNP)], (new, JJ...</td>\n      <td>F</td>\n    </tr>\n    <tr>\n      <th>165820</th>\n      <td>tt9914522</td>\n      <td>Evan</td>\n      <td>[make, regarding]</td>\n      <td>[new]</td>\n      <td>[Mackenzie, husband, decision]</td>\n      <td>[[(Mackenzie, NNP)], (new, JJ), (husband, NN),...</td>\n      <td>M</td>\n    </tr>\n  </tbody>\n</table>\n<p>165821 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "d751c18f-d88d-4723-b382-f30e3ea0bde4",
   "metadata": {},
   "source": [
    "previous cell gives df with columns IMDB_ID\tcharacter_name\tVerbs\tAdjectives\tNouns\tChunks\tgender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "characters_data=pd.read_csv('DATA/characters_data.csv',low_memory=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcd2c931-00b9-4f45-9384-33de2a4afb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save_df = pd.merge(characters_data, final_df, on=['IMDB_ID', 'character_name'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "        character_ID  wikipedia_ID freebase_ID actor_ethnicity  \\\n0                  1      975900.0   /m/03vyhn             NaN   \n1                  2      975900.0   /m/03vyhn      /m/044038p   \n2                  3      975900.0   /m/03vyhn         /m/0x67   \n3                  4      975900.0   /m/03vyhn             NaN   \n4                  5      975900.0   /m/03vyhn             NaN   \n...              ...           ...         ...             ...   \n336453        336454           NaN         NaN             NaN   \n336454        336455           NaN         NaN             NaN   \n336455        336456           NaN         NaN             NaN   \n336456        336457           NaN         NaN             NaN   \n336457        336458           NaN         NaN             NaN   \n\n                actor_name personnas    IMDB_ID              character_name  \\\n0           wanda de jesus       NaN  tt0228333                    Akooshay   \n1       natasha henstridge       NaN  tt0228333  Lieutenant Melanie Ballard   \n2                 ice cube       NaN  tt0228333         Desolation Williams   \n3            jason statham       NaN  tt0228333          Sgt Jericho Butler   \n4              clea duvall       NaN  tt0228333             Bashira Kincaid   \n...                    ...       ...        ...                         ...   \n336453       caleb silvers       NaN  tt9914522                        Evan   \n336454    bethany hazelitt       NaN  tt9914522                   Mackenzie   \n336455        joshua bootz       NaN  tt9914522                  Evan's dad   \n336456         vince camaj       NaN  tt9914522                        Todd   \n336457      sandra gendjar       NaN  tt9914522                      Kelsey   \n\n       actor_gender  box_office_revenue                    name  release_date  \\\n0                 F                 NaN          Ghosts of Mars        2001.0   \n1                 F          14010832.0          Ghosts of Mars        2001.0   \n2                 M          14010832.0          Ghosts of Mars        2001.0   \n3                 M          14010832.0          Ghosts of Mars        2001.0   \n4                 F                 NaN          Ghosts of Mars        2001.0   \n...             ...                 ...                     ...           ...   \n336453            M                 NaN  The Holden Family Plan        2019.0   \n336454            F                 NaN  The Holden Family Plan        2019.0   \n336455            M                 NaN  The Holden Family Plan        2019.0   \n336456            M                 NaN  The Holden Family Plan        2019.0   \n336457            F                 NaN  The Holden Family Plan        2019.0   \n\n        actor_age                                              Verbs  \\\n0            42.0                       [discovered, created, wiped]   \n1            27.0  [terraformed, allowing, become, authority, sen...   \n2            32.0  [wearing, become, named, named, held, mining, ...   \n3            34.0  [walk, wearing, sent, opened, released, posses...   \n4            23.0  [killed, returning, blame, cot, escapes, leaving]   \n...           ...                                                ...   \n336453       24.0                                  [make, regarding]   \n336454       29.0             [murdered, regarding, make, regarding]   \n336455        NaN                       [make, regarding, regarding]   \n336456        NaN                                                NaN   \n336457        NaN                                                NaN   \n\n                                               Adjectives  \\\n0                         [second, 22nd, ancient, fierce]   \n1       [matriarchal, police, second, second, small, s...   \n2       [transport, remote, ancient, horrific, team, d...   \n3       [second, pick, disembodied, possible, Unfortun...   \n4                                              [massacre]   \n...                                                   ...   \n336453                                              [new]   \n336454                                       [new, niece]   \n336455                                              [new]   \n336456                                                NaN   \n336457                                                NaN   \n\n                                                    Nouns  \\\n0       [Set, century, film, miners, Martian, miners, ...   \n1       [film, depicts, Mars, positions, story, concer...   \n2       [pressure, suits, society, prisoner, Williams,...   \n3       [half, humans, surface, pressure, suits, team,...   \n4       [pick, transport, prisoner, Desolation, Willia...   \n...                                                   ...   \n336453                     [Mackenzie, husband, decision]   \n336454  [Holden, sister, brother, law, husband, Evan, ...   \n336455        [Mackenzie, husband, decision, make, niece]   \n336456                                                NaN   \n336457                                                NaN   \n\n                                                   Chunks gender  \n0       [[(Set, NNP)], (second, JJ), (22nd, JJ), (cent...      F  \n1       [(film, NN), (depicts, NNS), (Mars, NNP), (84,...      F  \n2       [(wearing, VBG), (pressure, NN), (suits, NNS),...      M  \n3       [(second, JJ), (half, NN), (22nd, CD), (humans...      M  \n4       [(pick, NN), (transport, NN), (prisoner, NN), ...      F  \n...                                                   ...    ...  \n336453  [[(Mackenzie, NNP)], (new, JJ), (husband, NN),...      M  \n336454  [[(Holden, NNP)], (sister, NN), (brother, NN),...      F  \n336455  [[(Mackenzie, NNP)], (new, JJ), (husband, NN),...      M  \n336456                                                NaN    NaN  \n336457                                                NaN    NaN  \n\n[336458 rows x 18 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>character_ID</th>\n      <th>wikipedia_ID</th>\n      <th>freebase_ID</th>\n      <th>actor_ethnicity</th>\n      <th>actor_name</th>\n      <th>personnas</th>\n      <th>IMDB_ID</th>\n      <th>character_name</th>\n      <th>actor_gender</th>\n      <th>box_office_revenue</th>\n      <th>name</th>\n      <th>release_date</th>\n      <th>actor_age</th>\n      <th>Verbs</th>\n      <th>Adjectives</th>\n      <th>Nouns</th>\n      <th>Chunks</th>\n      <th>gender</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>975900.0</td>\n      <td>/m/03vyhn</td>\n      <td>NaN</td>\n      <td>wanda de jesus</td>\n      <td>NaN</td>\n      <td>tt0228333</td>\n      <td>Akooshay</td>\n      <td>F</td>\n      <td>NaN</td>\n      <td>Ghosts of Mars</td>\n      <td>2001.0</td>\n      <td>42.0</td>\n      <td>[discovered, created, wiped]</td>\n      <td>[second, 22nd, ancient, fierce]</td>\n      <td>[Set, century, film, miners, Martian, miners, ...</td>\n      <td>[[(Set, NNP)], (second, JJ), (22nd, JJ), (cent...</td>\n      <td>F</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>975900.0</td>\n      <td>/m/03vyhn</td>\n      <td>/m/044038p</td>\n      <td>natasha henstridge</td>\n      <td>NaN</td>\n      <td>tt0228333</td>\n      <td>Lieutenant Melanie Ballard</td>\n      <td>F</td>\n      <td>14010832.0</td>\n      <td>Ghosts of Mars</td>\n      <td>2001.0</td>\n      <td>27.0</td>\n      <td>[terraformed, allowing, become, authority, sen...</td>\n      <td>[matriarchal, police, second, second, small, s...</td>\n      <td>[film, depicts, Mars, positions, story, concer...</td>\n      <td>[(film, NN), (depicts, NNS), (Mars, NNP), (84,...</td>\n      <td>F</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>975900.0</td>\n      <td>/m/03vyhn</td>\n      <td>/m/0x67</td>\n      <td>ice cube</td>\n      <td>NaN</td>\n      <td>tt0228333</td>\n      <td>Desolation Williams</td>\n      <td>M</td>\n      <td>14010832.0</td>\n      <td>Ghosts of Mars</td>\n      <td>2001.0</td>\n      <td>32.0</td>\n      <td>[wearing, become, named, named, held, mining, ...</td>\n      <td>[transport, remote, ancient, horrific, team, d...</td>\n      <td>[pressure, suits, society, prisoner, Williams,...</td>\n      <td>[(wearing, VBG), (pressure, NN), (suits, NNS),...</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>975900.0</td>\n      <td>/m/03vyhn</td>\n      <td>NaN</td>\n      <td>jason statham</td>\n      <td>NaN</td>\n      <td>tt0228333</td>\n      <td>Sgt Jericho Butler</td>\n      <td>M</td>\n      <td>14010832.0</td>\n      <td>Ghosts of Mars</td>\n      <td>2001.0</td>\n      <td>34.0</td>\n      <td>[walk, wearing, sent, opened, released, posses...</td>\n      <td>[second, pick, disembodied, possible, Unfortun...</td>\n      <td>[half, humans, surface, pressure, suits, team,...</td>\n      <td>[(second, JJ), (half, NN), (22nd, CD), (humans...</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>975900.0</td>\n      <td>/m/03vyhn</td>\n      <td>NaN</td>\n      <td>clea duvall</td>\n      <td>NaN</td>\n      <td>tt0228333</td>\n      <td>Bashira Kincaid</td>\n      <td>F</td>\n      <td>NaN</td>\n      <td>Ghosts of Mars</td>\n      <td>2001.0</td>\n      <td>23.0</td>\n      <td>[killed, returning, blame, cot, escapes, leaving]</td>\n      <td>[massacre]</td>\n      <td>[pick, transport, prisoner, Desolation, Willia...</td>\n      <td>[(pick, NN), (transport, NN), (prisoner, NN), ...</td>\n      <td>F</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>336453</th>\n      <td>336454</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>caleb silvers</td>\n      <td>NaN</td>\n      <td>tt9914522</td>\n      <td>Evan</td>\n      <td>M</td>\n      <td>NaN</td>\n      <td>The Holden Family Plan</td>\n      <td>2019.0</td>\n      <td>24.0</td>\n      <td>[make, regarding]</td>\n      <td>[new]</td>\n      <td>[Mackenzie, husband, decision]</td>\n      <td>[[(Mackenzie, NNP)], (new, JJ), (husband, NN),...</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>336454</th>\n      <td>336455</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>bethany hazelitt</td>\n      <td>NaN</td>\n      <td>tt9914522</td>\n      <td>Mackenzie</td>\n      <td>F</td>\n      <td>NaN</td>\n      <td>The Holden Family Plan</td>\n      <td>2019.0</td>\n      <td>29.0</td>\n      <td>[murdered, regarding, make, regarding]</td>\n      <td>[new, niece]</td>\n      <td>[Holden, sister, brother, law, husband, Evan, ...</td>\n      <td>[[(Holden, NNP)], (sister, NN), (brother, NN),...</td>\n      <td>F</td>\n    </tr>\n    <tr>\n      <th>336455</th>\n      <td>336456</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>joshua bootz</td>\n      <td>NaN</td>\n      <td>tt9914522</td>\n      <td>Evan's dad</td>\n      <td>M</td>\n      <td>NaN</td>\n      <td>The Holden Family Plan</td>\n      <td>2019.0</td>\n      <td>NaN</td>\n      <td>[make, regarding, regarding]</td>\n      <td>[new]</td>\n      <td>[Mackenzie, husband, decision, make, niece]</td>\n      <td>[[(Mackenzie, NNP)], (new, JJ), (husband, NN),...</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>336456</th>\n      <td>336457</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>vince camaj</td>\n      <td>NaN</td>\n      <td>tt9914522</td>\n      <td>Todd</td>\n      <td>M</td>\n      <td>NaN</td>\n      <td>The Holden Family Plan</td>\n      <td>2019.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>336457</th>\n      <td>336458</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>sandra gendjar</td>\n      <td>NaN</td>\n      <td>tt9914522</td>\n      <td>Kelsey</td>\n      <td>F</td>\n      <td>NaN</td>\n      <td>The Holden Family Plan</td>\n      <td>2019.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>336458 rows × 18 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_save_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d23170a1-b722-4d69-bbd2-13e6ec621c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save_df.to_csv('DATA/characters_personas_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10e33ac-2601-479b-87ef-7a9b553ff075",
   "metadata": {},
   "source": [
    "### CA DEVRAIT MARCHER !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checking sizes before and after extract_words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 176334 after: 165821 before-after: 10513 number of empty rows: 10513\n"
     ]
    }
   ],
   "source": [
    "before=len(output_characters_genders_context)\n",
    "after=len(final_df)\n",
    "print('before:',before,'after:',after,'before-after:',before-after,'number of empty rows:',count_empty_rows)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This explains the difference in sizes before and after extract_words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
